{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taming systematic uncertainties\n",
    "\n",
    "In this notebook, we will demonstrate how one can model their systematic uncertainties by adding nuisance parameters to their MLE. Things are not implemented here efficiently, but instructively. That means we typing out a lot of equations without taking shortcuts. This should make it easier to follow along, but also makes it in some cases computationally slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "import scipy.integrate\n",
    "import numdifftools\n",
    "from tqdm import tqdm\n",
    "from uncertainties import ufloat\n",
    "from copy import deepcopy\n",
    "\n",
    "import plothist\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(DIRNAME)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.seterr(all='ignore');  # For a nicer saved notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an exponential distribution $\\mathcal{E}(2.0)$ as background and a normal distribution $\\mathcal{N}(3.0, 0.5)$ as signal distribution to generate our \"MC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = [\n",
    "        { \"type\": np.random.exponential, \"kwargs\": { \"scale\": 2.0 }},\n",
    "        { \"type\": np.random.normal,  \"kwargs\": { \"loc\": 3.0, \"scale\": 0.5 }},\n",
    "    ]\n",
    "\n",
    "def generate_distribution(N, distributions, component=None):\n",
    "    coefficients = np.array([0.88, 0.12])\n",
    "    coefficients /= coefficients.sum()\n",
    "\n",
    "    num_distr = len(distributions)\n",
    "\n",
    "    data = np.zeros((N, num_distr))\n",
    "\n",
    "    for idx, distr in enumerate(distributions):\n",
    "        data[:, idx] = distr[\"type\"](size=(N,), **distr[\"kwargs\"])\n",
    "    random_idx = np.random.choice(np.arange(num_distr), size=(N,), p=coefficients)\n",
    "\n",
    "    if component is None:\n",
    "        return data[np.arange(N), random_idx]\n",
    "    else:\n",
    "        return data[np.arange(N), component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "n_bkg = 900000\n",
    "n_sig = 100000\n",
    "\n",
    "data = generate_distribution(n_data, distributions, None)\n",
    "bkg_mc = generate_distribution(n_bkg, distributions, 0)\n",
    "sig_mc = generate_distribution(n_sig, distributions, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with binned data, so let us bin the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = (0, 6)\n",
    "bins=20\n",
    "template_data, template_data_boundaries = np.histogram(data, bins=bins, range=x_range)\n",
    "template_bkg, template_bkg_boundaries = np.histogram(bkg_mc, bins=bins, range=x_range, \n",
    "                                                     weights=np.ones(len(bkg_mc))/100)\n",
    "template_sig, template_sig_boundaries = np.histogram(sig_mc, bins=bins, range=x_range, \n",
    "                                                     weights=np.ones(len(sig_mc))/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_bkg_expected = sum(template_bkg)\n",
    "nu_sig_expected = sum(template_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    template_data_boundaries[:-1] + (template_data_boundaries[1] - template_data_boundaries[0])/2,\n",
    "    template_data,\n",
    "    yerr=np.sqrt(template_data),\n",
    "    label=\"Data\", ls='', marker='.', color='black'\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    template_bkg_boundaries[:-1] + (template_bkg_boundaries[1] - template_bkg_boundaries[0])/2,\n",
    "    template_bkg,\n",
    "    width=(template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    label='Background Template', \n",
    ")\n",
    "\n",
    "plt.plot(np.linspace(0, 6), \n",
    "         sum(template_bkg) * ((max(x_range) - min(x_range)) / bins) * scipy.stats.expon.pdf(np.linspace(0, 6), scale=2), \n",
    "         label='Background pdf', lw=2)\n",
    "\n",
    "plt.bar(\n",
    "    template_sig_boundaries[:-1] + (template_sig_boundaries[1] - template_sig_boundaries[0])/2,\n",
    "    template_sig, \n",
    "    width=(template_sig_boundaries[1] - template_sig_boundaries[0]),\n",
    "    label='Signal Template', \n",
    ")\n",
    "\n",
    "plt.plot(np.linspace(0, 6), \n",
    "         sum(template_sig) * ((max(x_range) - min(x_range)) / bins) * scipy.stats.norm.pdf(np.linspace(0, 6), loc=3, scale=0.5), \n",
    "         label='Signal pdf', lw=2)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Measured Quantity\")\n",
    "plt.ylabel(\"Entries\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1300)\n",
    "plt.xlim(0,6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(DIRNAME,\"pdf-to-template.png\"), transparent=True, bbox_inches='tight')\n",
    "plt.savefig(os.path.join(DIRNAME,\"pdf-to-template.pdf\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(\n",
    "    template_data_boundaries[:-1] + (template_data_boundaries[1] - template_data_boundaries[0])/2,\n",
    "    template_data,\n",
    "    yerr=np.sqrt(template_data),\n",
    "    label=\"Data\", ls='', marker='.', color='black'\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    template_bkg_boundaries[:-1] + (template_bkg_boundaries[1] - template_bkg_boundaries[0])/2,\n",
    "    template_bkg,\n",
    "    width=(template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    label='Background Template', \n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    template_sig_boundaries[:-1] + (template_sig_boundaries[1] - template_sig_boundaries[0])/2,\n",
    "    template_sig, bottom=template_bkg,\n",
    "    width=(template_sig_boundaries[1] - template_sig_boundaries[0]),\n",
    "    label='Signal Template', \n",
    ")\n",
    "\n",
    "plt.plot(np.linspace(0, 6), \n",
    "         [sum(template_bkg) * ((max(x_range) - min(x_range)) / bins) * scipy.stats.expon.pdf(x, scale=2) + sum(template_sig) * ((max(x_range) - min(x_range)) / bins) * scipy.stats.norm.pdf(x, loc=3, scale=0.5)\n",
    "          for x in np.linspace(0, 6)],\n",
    "         label='Full Model', lw=2, color='black')\n",
    "\n",
    "plt.xlabel(\"Measured Quantity\")\n",
    "plt.ylabel(\"Entries\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1300)\n",
    "plt.xlim(0,6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(DIRNAME,\"pdf-to-template2.png\"), transparent=True, bbox_inches='tight')\n",
    "plt.savefig(os.path.join(DIRNAME,\"pdf-to-template2.pdf\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_uncertainty(x):\n",
    "    x[x==0] = 1e-7\n",
    "    return x\n",
    "\n",
    "def correlation_from_covariance(x):\n",
    "    return x / np.outer(np.sqrt(x.diagonal()), np.sqrt(x.diagonal()))\n",
    "\n",
    "template_bkg_stat, _ = np.histogram(bkg_mc, bins=bins, range=x_range,\n",
    "                               weights=(np.ones(len(bkg_mc))/100)**2)\n",
    "template_sig_stat, _ = np.histogram(sig_mc, bins=bins, range=x_range, \n",
    "                               weights=(np.ones(len(sig_mc))/100)**2)\n",
    "\n",
    "template_bkg_stat = sanitize_uncertainty(template_bkg_stat)\n",
    "template_sig_stat = sanitize_uncertainty(template_sig_stat)\n",
    "\n",
    "bkg_stat_covariance = np.diag(template_bkg_stat**2)\n",
    "sig_stat_covariance = np.diag(template_sig_stat**2)\n",
    "\n",
    "def get_relative_error_and_inv_correlation(template, covariance):\n",
    "    uncertainty_per_bin = np.diag(covariance)**0.5\n",
    "    relative_error = np.nan_to_num(uncertainty_per_bin / template, posinf=1e-7)\n",
    "    correlation = correlation_from_covariance(covariance)\n",
    "    return relative_error, np.linalg.inv(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the inverse of the correlations once, and reuse that later, instead of doing this in each step of the Likelihood minimization.\n",
    "# (It is computational very expensive)\n",
    "template_bkg_relative_errors, inv_bkg_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_bkg, bkg_stat_covariance)\n",
    "template_sig_relative_errors, inv_sig_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_sig, sig_stat_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_template(template_data, template_bkg, template_sig, fit_result=None, debug=False, figname=None):\n",
    "\n",
    "    plt.errorbar(\n",
    "        template_data_boundaries[:-1] + (template_data_boundaries[1] - template_data_boundaries[0])/2,\n",
    "        template_data,\n",
    "        yerr=np.sqrt(template_data),\n",
    "        label=\"Data\", ls='', marker='.', color='black'\n",
    "    )\n",
    "    \n",
    "    copy_bkg_template = deepcopy(template_bkg)\n",
    "    copy_sig_template = deepcopy(template_sig)\n",
    "    if fit_result is not None:\n",
    "        \n",
    "        nBins = 20\n",
    "        nParamaterOfInterest = 2\n",
    "        nParameterOfNuisance = 2 * nBins\n",
    "        par_interest = fit_result.x[:nParamaterOfInterest]\n",
    "        par_nuisance = fit_result.x[nParamaterOfInterest:nParamaterOfInterest+nParameterOfNuisance]\n",
    "    \n",
    "        copy_bkg_template *= par_interest[0] / sum(copy_bkg_template)\n",
    "        copy_sig_template *= par_interest[1] / sum(copy_sig_template)\n",
    "        if len(par_nuisance) == 0:\n",
    "            par_nuisance = np.zeros(nParameterOfNuisance)\n",
    "        copy_bkg_template *= (1 + par_nuisance[:bins] * template_bkg_relative_errors)\n",
    "        copy_sig_template *= (1 + par_nuisance[bins:] * template_sig_relative_errors)\n",
    "    \n",
    "    plt.bar(\n",
    "        template_bkg_boundaries[:-1] + (template_bkg_boundaries[1] - template_bkg_boundaries[0])/2,\n",
    "        copy_bkg_template,\n",
    "        width=(template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "        label='Background'\n",
    "    )\n",
    "\n",
    "    plt.bar(\n",
    "        template_sig_boundaries[:-1] + (template_sig_boundaries[1] - template_sig_boundaries[0])/2,\n",
    "        copy_sig_template, bottom=copy_bkg_template,\n",
    "        width=(template_sig_boundaries[1] - template_sig_boundaries[0]),\n",
    "        label=\"Signal\",\n",
    "    )\n",
    "        \n",
    "    plt.xlabel(\"Measured Quantity\")\n",
    "    plt.ylabel(\"Entries\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,1300)\n",
    "    plt.xlim(0,6)\n",
    "    plt.tight_layout()\n",
    "    if figname is not None:\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".png\"), transparent=True, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".pdf\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    if debug:\n",
    "        print(copy_bkg_template)\n",
    "        print(copy_sig_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimov Data: A dataset defined to be the same as the expectation\n",
    "\n",
    "Data: The dataset from our (pseudo)experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asimov_data = template_sig + template_bkg\n",
    "template_asimov = np.round(asimov_data).astype(int)\n",
    "\n",
    "sample = (template_data, template_bkg, template_sig)  # data\n",
    "# sample = (asimov_data, template_bkg, template_sig)  # asimov_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_template(*sample, figname='initial-problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 1: Implement a likelihood function\n",
    "\n",
    "We have our recorded data as black data points, and our signal and background templates in orange and blue. We want to extract the signal yields now. For this we deploy the method of Maximum Likelihood: $\\mathcal{L} = \\prod_i^{\\text{nbins}} \\mathcal{P}(n_i | \\nu_i)$\n",
    "\n",
    "We have $\\mathcal{P}$ the Poisson distribution where $n_i$ is the number of measured data events in bin $i$, and $\\nu_i$ the total number of expected events in bin $i$. \n",
    "\n",
    "$\\nu_i$ = $\\sum_k^\\text{ntemplates} f_{ik} \\eta_k$, \n",
    "\n",
    "where $\\eta_k$ is the total number of events expected from template $k$ and $f_{ik}$ is the fraction of events expected in bin $i$ of template $k$.\n",
    "\n",
    "For convenience (and computational stability) we use\n",
    "\n",
    "$-2 \\ln \\mathcal{L} = -2 \\sum_i^{\\text{nbins}} \\ln \\mathcal{P}(n_i | \\nu_i)$\n",
    "\n",
    "So far, we did not include any (systematic) uncertainty into our fit model (the templates). Let us assume we know that our background template depends on a parameter $\\lambda$ and this parameter has some model uncertainty. You got this from e.g. the performace group or it is an uncertainty in a theoretical model. Anyway, we need to include this uncertainty into our template!\n",
    "\n",
    "The first one we are going to include is the uncertainty from limited MC statistics. This also helps us to avoid problems which arise when the covariance matrix is 100% correlated (like in the following). For that we simply assign the statistical uncertainty of the MC templates in each bin via $\\sigma=\\sum_i w_i^2$, where $w_i$ is the weight of the provided MC.\n",
    "\n",
    "As this error originates from statistical uncertainty, the errors are uncorrelated in each bin. We build a covariance by $\\mathcal{C} = diag(\\sigma_i)$ where $\\sigma_i$ is the uncertainty in each bin. We construct this for the signal an background template: $\\mathcal{C}_\\text{stat}^\\text{sig}$, $\\mathcal{C}_\\text{stat}^\\text{bkg}$, \n",
    "\n",
    "To include this uncertainty into the fit, we modify our likelihood and introduce nuisance parameters:\n",
    "\n",
    "$\\mathcal{L} = \\prod_i^{\\text{nbins}} \\mathcal{P}(n_i | \\nu_i(\\theta)) \\times \\prod_k^\\text{templates} \\mathcal{N}(\\theta_k|0, \\mathcal{\\Sigma}^k$)\n",
    "\n",
    "The normal distribution is utilized to allow the nuisance parameter to vary within the uncertainty. $\\Sigma_k$ is the total correlation matrix for any template. Currently this consists only of the statistical uncertainty for the signal and for the background template.\n",
    "$\\theta$ is the vector of nuisance parameters, $\\theta_k$ the vector of nuisance parameters associated with template $k$.\n",
    "\n",
    "As we can now vary individual bins within their uncertainty, the fraction of expected events in each bin is not constant, but varies!\n",
    "\n",
    "$f_{ik} = \\frac{\\eta_{ik} (1 + \\theta_{ik} \\epsilon_{ik})}{\\sum_k^\\text{bins} \\eta_{jk} (1 + \\theta_{jk} \\epsilon_{jk})}$.\n",
    "\n",
    "Writing it like this ensures that pulling at a nuisance paramter does not change the normalization $\\nu$. Indicies $i$: bin, $k$:template. $\\epsilon$ is the relative uncertainty in of template $k$ in bin $i$. $\\theta_{ik}$ is the nuisance parameter.\n",
    "\n",
    "Now our likelihood depends on 2 parameters of interest: $\\nu_\\text{sig}$ and $\\nu_\\text{bkg}$ and a total of $(k=2) * (bins=30)$ nuisance parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def likelihood(x, \n",
    "               bkg_errors, sig_errors, \n",
    "               inv_bkg_correlation, inv_sig_correlation, \n",
    "               *pars):\n",
    "    \"\"\"We pass arguments here, their meaning becomes clearer when unpacked. See the following lines.\"\"\"\n",
    "    \n",
    "    data, bkg, sig = x  \n",
    "    # data: Data histogram \n",
    "    # bkg: MC background histogram\n",
    "    # sig: MC signal histogram\n",
    "\n",
    "    nBins = 20\n",
    "    nParamaterOfInterest = 2\n",
    "    nParameterOfNuisance = 2 * nBins\n",
    "    par_interest = pars[:nParamaterOfInterest]  # These are the yields, nu_sig and nu_bkg\n",
    "    par_nuisance = pars[nParamaterOfInterest:nParamaterOfInterest+nParameterOfNuisance]  # These are the nuisance parameters for the shape uncertainties\n",
    "    \n",
    "    use_nuisance = True\n",
    "    if len(par_nuisance) == 0: \n",
    "        par_nuisance = np.zeros(nParameterOfNuisance)\n",
    "        use_nuisance = False\n",
    "        \n",
    "    par_nuisance_bkg = par_nuisance[:nBins]\n",
    "    par_nuisance_sig = par_nuisance[nBins:]\n",
    "    \n",
    "    # Calculate the fraction of events we expect in each bin, either with or without nuisance parameters.\n",
    "    # The signal and background errors are passed as an array to the function\n",
    "    if use_nuisance:\n",
    "        fractiong_bkg = bkg * (1 + par_nuisance_bkg * bkg_errors) / sum(bkg * (1 + par_nuisance_bkg * bkg_errors))\n",
    "        fractions_sig = sig * (1 + par_nuisance_sig * sig_errors) / sum(sig * (1 + par_nuisance_sig * sig_errors))\n",
    "    else:\n",
    "        fractiong_bkg = bkg / sum(bkg)\n",
    "        fractions_sig = sig / sum(sig)\n",
    "\n",
    "    # Group it to a a list\n",
    "    eta_k = [fractiong_bkg, fractions_sig]\n",
    "        \n",
    "    # Calculate the signal and background expectation in each bin\n",
    "    nu_0_per_bin = par_interest[0] * eta_k[0]\n",
    "    nu_1_per_bin = par_interest[1] * eta_k[1]\n",
    "    nu_per_bin = [nu_0_per_bin, nu_1_per_bin]\n",
    "    \n",
    "    # We have now calculated /retrieved all inidividual parts we required to calculate the likelihood.\n",
    "    # nu_per_bin is the expectation per bin\n",
    "    # data is the observed data in each bin\n",
    "    # inv_sig(bkg)_correlation was calculated once outside the function, but is passed to the function\n",
    "    # Return the -2 ln likelihood, either with or without nuisance parameters.\n",
    "    # (Although the function is called likelihood, return the -2ln likelihood)\n",
    "    if use_nuisance:\n",
    "        return -2 * sum(np.log(scipy.stats.poisson.pmf(data, sum(nu_per_bin)))) \\\n",
    "                + par_nuisance_bkg @ inv_bkg_correlation @ par_nuisance_bkg \\\n",
    "                + par_nuisance_sig @ inv_sig_correlation @ par_nuisance_sig\n",
    "    else:\n",
    "        return -2 * sum(np.log(scipy.stats.poisson.pmf(data, sum(nu_per_bin))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(x, *pars):\n",
    "    \"\"\"Just a wrapper to make function calls easier (cache some arguments).\"\"\"\n",
    "    return likelihood(x, \n",
    "                      template_bkg_relative_errors, template_sig_relative_errors, \n",
    "                      inv_bkg_correlation, inv_sig_correlation,\n",
    "                      *pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call it once without nuisance parameters,\n",
    "L(sample, nu_bkg_expected, nu_sig_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and once with nuisance parameters to be fitted\n",
    "L(sample, nu_bkg_expected, nu_sig_expected, *np.zeros(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, if we chance the nuisance parameters, then the likelihood changes\n",
    "L(sample, nu_bkg_expected, nu_sig_expected, *np.ones(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the fit on the data, to extract the parameters of interest, nu_bkg and nu_sig. To estimate the error, we can either use the Hesse approximation at the minimum of the Likelihood or perform a profile Likelihood scan.\n",
    "The first one will automatically give us something which we can interpret at the covariance, the second one allows for asymetric errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = scipy.optimize.minimize(\n",
    "    lambda x: L(sample, *x),\n",
    "    x0=(nu_bkg_expected, nu_sig_expected),\n",
    "    method=\"SLSQP\"\n",
    ")\n",
    "\n",
    "hesse = numdifftools.Hessian(\n",
    "    lambda x: L(sample, *x))(fit_result.x)\n",
    "\n",
    "fit_result.covariance =  np.linalg.inv(hesse / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fit_result(fit_result):\n",
    "    print(\"\"\"\n",
    "    nu(bkg) = {:.2f}\n",
    "    nu(sig) = {:.2f}\n",
    "    \"\"\".format(\n",
    "        ufloat(fit_result.x[0], fit_result.covariance[0, 0]**0.5),\n",
    "        ufloat(fit_result.x[1], fit_result.covariance[1, 1]**0.5)\n",
    "    ))\n",
    "\n",
    "print_fit_result(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_stat_only = deepcopy(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_range(fit_result, npar):\n",
    "    return np.linspace(\n",
    "        fit_result.x[npar] - 1.5 * fit_result.covariance[npar, npar]**0.5,\n",
    "        fit_result.x[npar] + 1.5 * fit_result.covariance[npar, npar]**0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_scan(fit_result, ipar):\n",
    "\n",
    "    fit_result_profile_scan = []\n",
    "    for value in get_plot_range(fit_result, ipar):\n",
    "        fit_result_profile = scipy.optimize.minimize(\n",
    "            lambda x: L(sample, *x),\n",
    "            x0=fit_result.x,\n",
    "            method=\"SLSQP\",\n",
    "            constraints={'type': 'eq', 'fun': lambda x: x[ipar] - value}\n",
    "        )\n",
    "        fit_result_profile_scan.append(fit_result_profile)\n",
    "    \n",
    "    try: \n",
    "        fit_result_profile_scan_stat_only = []\n",
    "        for value in get_plot_range(fit_result, ipar):\n",
    "            fit_result_profile = scipy.optimize.minimize(\n",
    "                lambda x: L(sample, *x, *fit_result.x[2:]),\n",
    "                x0=fit_result.x[:2],\n",
    "                method=\"SLSQP\",\n",
    "                constraints={'type': 'eq', 'fun': lambda x: x[ipar] - value}\n",
    "            )\n",
    "            fit_result_profile_scan_stat_only.append(fit_result_profile)\n",
    "        return fit_result_profile_scan, fit_result_profile_scan_stat_only            \n",
    "    except IndexError:\n",
    "        return fit_result_profile_scan, None\n",
    "                    \n",
    "    \n",
    "\n",
    "\n",
    "def plot_likelihood_scan(fit_result, fit_result_profile_scan, fit_result_profile_scan_stat_only, ipar, figname):\n",
    "    plt.plot(\n",
    "        get_plot_range(fit_result, ipar), \n",
    "        [fr.fun - fit_result.fun for fr in fit_result_profile_scan],\n",
    "        label=\"Likelihood profile\"\n",
    "    )\n",
    "    if fit_result_profile_scan_stat_only is not None:\n",
    "        plt.plot(\n",
    "            get_plot_range(fit_result, ipar), \n",
    "            [fr.fun - fit_result.fun for fr in fit_result_profile_scan_stat_only],\n",
    "            label=\"stat. only\", ls='dotted'\n",
    "        )\n",
    "    plt.errorbar(fit_result.x[ipar], 1, \n",
    "                 xerr=fit_result.covariance[ipar, ipar]**0.5,\n",
    "                color='orange', marker='o', ls='-', lw=2,\n",
    "                label=\"Gaussian approximation\")\n",
    "\n",
    "    subscript=1\n",
    "    if ipar == 0:\n",
    "        subscript=2\n",
    "    \n",
    "    plt.xlabel(r\"$\\eta_{\"+f\"_{subscript}\"+\"}$\")\n",
    "    if ipar==42:\n",
    "        plt.xlabel(r\"$\\theta_{s}$\")\n",
    "    plt.ylabel(r\"$-2\\ln{(\\mathcal{L/L}_\\text{min})}$\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,2.5)\n",
    "    # plt.xlim(0,6)\n",
    "    plt.tight_layout();\n",
    "    if figname is not None:\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".png\"), transparent=True, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".pdf\"))\n",
    "    plt.show()        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 0), 0, figname=\"profile-statonly-ipar0\")\n",
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 1), 1, figname=\"profile-statonly-ipar1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_template(*sample, fit_result=fit_result, debug=False, figname=\"template-statonly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toy(nToys):\n",
    "    toy_fit_results = []\n",
    "\n",
    "    for i in tqdm(range(nToys)):\n",
    "        template_toy = scipy.stats.poisson(template_asimov).rvs()\n",
    "        toy_sample = (template_toy, template_bkg, template_sig)\n",
    "        toy_result = scipy.optimize.minimize(\n",
    "            lambda x: L(toy_sample, *x),\n",
    "            x0=fit_result.x,\n",
    "            method=\"SLSQP\"\n",
    "        )\n",
    "        hesse = numdifftools.Hessian(lambda x: L(sample, *x))(toy_result.x)\n",
    "        toy_result.covariance =  np.linalg.inv(hesse / 2)\n",
    "        toy_fit_results.append(toy_result)\n",
    "        \n",
    "    return toy_fit_results\n",
    "        \n",
    "def plot_toy_fit_results(toy_fit_results, par, expected, figname=None):    \n",
    "\n",
    "    plt.plot(\n",
    "        np.linspace(-5, 5), \n",
    "        scipy.stats.norm.pdf(\n",
    "            np.linspace(-5, 5)\n",
    "        ),\n",
    "        ls='solid', marker='', label='Expected')\n",
    "    \n",
    "    plt.hist([(r.x[par] - expected) / r.covariance[par, par]**0.5 for r in toy_fit_results],\n",
    "             bins=20, range=(-5, 5), density=True, label=\"Toy Results\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Pull\")\n",
    "    plt.ylabel(\"Entries\")\n",
    "    plt.tight_layout();\n",
    "    if figname is not None:\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".png\"), transparent=True)\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".pdf\"))\n",
    "    plt.show()        \n",
    "    plt.close()\n",
    "    print(\"Toy result: {:.2f}\".format(ufloat(\n",
    "        np.mean([(r.x[par] - expected) / r.covariance[par, par]**0.5 for r in toy_fit_results]),\n",
    "        np.std([(r.x[par] - expected) / r.covariance[par, par]**0.5 for r in toy_fit_results])\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toy_fit_results = generate_toy(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_toy_fit_results(toy_fit_results, 0, nu_bkg_expected, \"toy-statonly-ipar0\")\n",
    "# plot_toy_fit_results(toy_fit_results, 1, nu_sig_expected, \"toy-statonly-ipar1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L(sample, *fit_result.x) - fit_result.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_bkg_relative_errors, inv_bkg_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_bkg, bkg_stat_covariance)\n",
    "template_sig_relative_errors, inv_sig_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_sig, sig_stat_covariance)\n",
    "\n",
    "def L(x, *pars):\n",
    "    return likelihood(x, \n",
    "                      template_bkg_relative_errors, template_sig_relative_errors, \n",
    "                      inv_bkg_correlation, inv_sig_correlation,\n",
    "                      *pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result = scipy.optimize.minimize(\n",
    "    lambda x: L(sample, *x),\n",
    "    x0=(nu_bkg_expected, nu_sig_expected, *np.zeros(40)),\n",
    "    method=\"SLSQP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hesse = numdifftools.Hessian(\n",
    "    lambda x: L(sample, *x))(fit_result.x)\n",
    "\n",
    "fit_result.covariance =  np.linalg.inv(hesse / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_fit_result(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_result_limited_mc_stat = deepcopy(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 0), 0, figname=\"profile-limitedmc-ipar0\")\n",
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 1), 1, figname=\"profile-limitedmc-ipar1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on which nuisance parameters the fit tried to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pulls_on_bins(fit_result, figname=None):\n",
    "\n",
    "    plt.errorbar(range(0, bins), fit_result.x[2:bins+2], yerr=1,\n",
    "                ls='', marker='.', label=\"Pull on Background Nuisance\"\n",
    "                )\n",
    "    plt.bar(range(0, bins), height=2*np.diagonal(fit_result.covariance[2:bins+2, 2:bins+2])**0.5,\n",
    "            bottom=fit_result.x[2:bins+2]-np.diagonal(fit_result.covariance[2:bins+2, 2:bins+2])**0.5,\n",
    "            width=1, alpha=0.1, label=\"Post-fit Errors\"\n",
    "           )\n",
    "    plt.errorbar(range(bins, 2*bins), fit_result.x[bins+2:2*bins+2], yerr=1,\n",
    "                ls='', marker='.', label='Pull on Signal Nuisance'\n",
    "                )\n",
    "    plt.bar(range(bins, 2*bins), height=2*np.diagonal(fit_result.covariance[bins+2:2*bins+2, bins+2:2*bins+2])**0.5,\n",
    "            bottom=fit_result.x[bins+2:2*bins+2]-np.diagonal(fit_result.covariance[bins+2:2*bins+2, bins+2:2*bins+2])**0.5,\n",
    "            width=1, alpha=0.1, label=\"Post-fit Errors\"\n",
    "           )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim(-2, 4)\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(\"Standard Deviations\")\n",
    "    plt.tight_layout();\n",
    "    if figname is not None:\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".png\"), transparent=True, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(DIRNAME, figname + \".pdf\"))\n",
    "    plt.show()        \n",
    "    plt.close()\n",
    "    \n",
    "plot_pulls_on_bins(fit_result, figname=\"nuisance-limitedmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_template(*sample, fit_result, debug=False, figname=\"template-limitedmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toy_fit_results = generate_toy(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_toy_fit_results(toy_fit_results, 0, nu_bkg_expected, \"toy-limitedmc-ipar0\")\n",
    "# plot_toy_fit_results(toy_fit_results, 1, nu_sig_expected, \"toy-limitedmc-ipar1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's include model systematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our background model can be written as $\\mathcal{E}(\\lambda=(2\\pm 0.2))$. We have our central value $\\lambda=2$ and an assoicated covariance matrix (in our case 1D) $\\mathcal{C} = (0.04)$.\n",
    "\n",
    "To incorporate this in our fit, we now determine how this systematic uncertainty can be built into our template.\n",
    "\n",
    "There are sometimes easier ways, e.g. via reweighting, but now we do the following:\n",
    "* Diagonalize the covariance matrix.\n",
    "* Vary each eigenvalue by +- 1 sigma.\n",
    "* For each variation, rotate it back and generate a new template.\n",
    "In our case this is easy, because it is only 1D! So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distributions_up = [\n",
    "        { \"type\": np.random.exponential, \"kwargs\": { \"scale\": 2.0 + 0.2 }},\n",
    "        { \"type\": np.random.normal,  \"kwargs\": { \"loc\": 3.5, \"scale\": 0.5 }},\n",
    "    ]\n",
    "\n",
    "template_bkg_plus, _ = np.histogram(\n",
    "    generate_distribution(n_bkg, distributions_up, 0),\n",
    "    bins=bins, range=x_range, \n",
    "    weights=np.ones(len(bkg_mc))/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distributions_down = [\n",
    "        { \"type\": np.random.exponential, \"kwargs\": { \"scale\": 2.0 - 0.2 }},\n",
    "        { \"type\": np.random.normal,  \"kwargs\": { \"loc\": 3.5, \"scale\": 0.5 }},\n",
    "    ]\n",
    "\n",
    "template_bkg_minus, _ = np.histogram(\n",
    "    generate_distribution(n_bkg, distributions_down, 0),\n",
    "    bins=bins, range=x_range, \n",
    "    weights=np.ones(len(bkg_mc))/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the nominal distribution, plus two distributions which are varied by the eigenvalue of the covariance matrix of the nuisance parameter.\n",
    "We know calculate how this variation impacts the expectation in each bin.\n",
    "We symmetrize the error here so we can use it as a covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    template_bkg_boundaries[:-1] + (template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    bins= template_bkg_boundaries + (template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    weights=template_bkg,    label=r'$\\lambda = 2.0$', histtype='step', color='black'\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    template_bkg_boundaries[:-1] + (template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    bins= template_bkg_boundaries + (template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    weights=template_bkg_minus,    label=r'$\\lambda = 1.8$', histtype='step'\n",
    ")\n",
    "\n",
    "plt.hist(\n",
    "    template_bkg_boundaries[:-1] + (template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    bins= template_bkg_boundaries + (template_bkg_boundaries[1] - template_bkg_boundaries[0]),\n",
    "    weights=template_bkg_plus,    label=r'$\\lambda = 2.2$', histtype='step'\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Measured Quantitiy\")\n",
    "plt.ylabel(\"Entries\")\n",
    "# plt.yscale('log')\n",
    "plt.xlim(0,6)\n",
    "plt.tight_layout();\n",
    "plt.savefig(os.path.join(DIRNAME, \"template-model-uncertainty.pdf\"))\n",
    "plt.savefig(os.path.join(DIRNAME, \"template-model-uncertainty.png\"), transparent=True, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_uncertainty(nominal, plus, minus):\n",
    "    down_variation = minus - nominal\n",
    "    up_variation = plus - nominal\n",
    "    model_uncertainty_per_bin = (np.sign(down_variation) * (abs(down_variation) + abs(up_variation))) / 2\n",
    "    return model_uncertainty_per_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_model_uncertainty(template_bkg, template_bkg_plus, template_bkg_minus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory error is 100% correlated, so we calculate the covariance as the outer product $\\sigma \\otimes \\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_uncertainty_per_bin = get_model_uncertainty(template_bkg, template_bkg_plus, template_bkg_minus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bkg_model_covariance = np.outer(model_uncertainty_per_bin, model_uncertainty_per_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply add up the uncertainties and covariance matrices, calculate the relative errors and correlation matrices anew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_bkg_relative_errors, inv_bkg_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_bkg, bkg_stat_covariance + bkg_model_covariance)\n",
    "template_sig_relative_errors, inv_sig_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_sig, sig_stat_covariance)\n",
    "\n",
    "def L(x, *pars):\n",
    "    return likelihood(x, \n",
    "                      template_bkg_relative_errors, template_sig_relative_errors, \n",
    "                      inv_bkg_correlation, inv_sig_correlation,\n",
    "                      *pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_result = scipy.optimize.minimize(\n",
    "    lambda x: L(sample, *x),\n",
    "    x0=(nu_bkg_expected, nu_sig_expected, *np.zeros(40)),\n",
    "    method=\"SLSQP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hesse = numdifftools.Hessian(\n",
    "    lambda x: L(sample, *x))(fit_result.x)\n",
    "\n",
    "fit_result.covariance =  np.linalg.inv(hesse / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_fit_result(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_result_model_sys = deepcopy(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 0), 0, figname=\"profile-modeluncertainty-ipar0\")\n",
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 1), 1, figname=\"profile-modeluncertainty-ipar1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_pulls_on_bins(fit_result, figname=\"nuisance-modeluncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_template(*sample, fit_result, debug=False, figname=\"template-modeluncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toy_fit_results = generate_toy(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_toy_fit_results(toy_fit_results, 0, nu_bkg_expected, \"toy-modeluncertainty-ipar0\")\n",
    "# plot_toy_fit_results(toy_fit_results, 1, nu_sig_expected, \"toy-modeluncertainty-ipar1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to incorporate the uncertainty of the tracking to our signal template. This works a bit differently, as it is a multiplicative uncertainty. To include this in the likelihood, we introduce a single nuisance parameter (assuming the tracking uncertainty is constant across our measured quanitity):\n",
    "\n",
    "$\\eta_k \\rightarrow \\eta_k (1 + \\epsilon_s \\theta_{ks}$)\n",
    "\n",
    "This introduces a single new nuisance parameter $\\theta_{ks}$ which allows to pull on the uncertainty $\\epsilon_s$ for template $k$.\n",
    "\n",
    "Let us assume we have a systematic error of 3\\% on the tracking efficiency, $\\epsilon_s = 0.03$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def likelihood(x, \n",
    "               bkg_errors, sig_errors,\n",
    "               inv_bkg_correlation, inv_sig_correlation, \n",
    "               multiplicative_uncertainties,\n",
    "               *pars):\n",
    "    data, bkg, sig = x\n",
    "\n",
    "    nBins = 20\n",
    "    nParamaterOfInterest = 2\n",
    "    nParameterOfNuisance = 2 * nBins\n",
    "    par_interest = np.array(pars[:nParamaterOfInterest])\n",
    "    par_nuisance = np.array(pars[nParamaterOfInterest:nParamaterOfInterest+nParameterOfNuisance])\n",
    "    par_nuisance_mult = np.array(pars[nParamaterOfInterest+nParameterOfNuisance:])\n",
    "    \n",
    "    for eps, th in zip(multiplicative_uncertainties, par_nuisance_mult):\n",
    "        par_interest[1] = par_interest[1] * (1 + eps * th)\n",
    "    \n",
    "    use_nuisance = True\n",
    "    if len(par_nuisance) == 0:\n",
    "        par_nuisance = np.zeros(nParameterOfNuisance)\n",
    "        use_nuisance = False\n",
    "        \n",
    "    par_nuisance_bkg = par_nuisance[:nBins]\n",
    "    par_nuisance_sig = par_nuisance[nBins:]\n",
    "    \n",
    "    if use_nuisance:\n",
    "        fractiong_bkg = bkg * (1 + par_nuisance_bkg * bkg_errors) / sum(bkg * (1 + par_nuisance_bkg * bkg_errors))\n",
    "        fractions_sig = sig * (1 + par_nuisance_sig * sig_errors) / sum(sig * (1 + par_nuisance_sig * sig_errors))\n",
    "    else:\n",
    "        fractiong_bkg = bkg / sum(bkg)\n",
    "        fractions_sig = sig / sum(sig)\n",
    "\n",
    "    eta_k = [fractiong_bkg, fractions_sig]\n",
    "    \n",
    "    nu_0_per_bin = par_interest[0] * eta_k[0]\n",
    "    nu_1_per_bin = par_interest[1] * eta_k[1]\n",
    "    nu_per_bin = [nu_0_per_bin, nu_1_per_bin]\n",
    "    \n",
    "    if use_nuisance:\n",
    "        return -2 * sum(np.log(scipy.stats.poisson.pmf(data, sum(nu_per_bin)))) \\\n",
    "                + par_nuisance_bkg @ inv_bkg_correlation @ par_nuisance_bkg \\\n",
    "                + par_nuisance_sig @ inv_sig_correlation @ par_nuisance_sig \\\n",
    "                + sum(par_nuisance_mult**2)\n",
    "    else:\n",
    "        return -2 * sum(np.log(scipy.stats.poisson.pmf(data, sum(nu_per_bin))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_bkg_relative_errors, inv_bkg_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_bkg, bkg_stat_covariance + bkg_model_covariance)\n",
    "template_sig_relative_errors, inv_sig_correlation = get_relative_error_and_inv_correlation(\n",
    "    template_sig, sig_stat_covariance)\n",
    "multiplicative_errors = np.array([0.03,])\n",
    "\n",
    "def L(x, *pars):\n",
    "    return likelihood(x, \n",
    "                      template_bkg_relative_errors, template_sig_relative_errors, \n",
    "                      inv_bkg_correlation, inv_sig_correlation,\n",
    "                      multiplicative_errors,\n",
    "                      *pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_result = scipy.optimize.minimize(\n",
    "    lambda x: L(sample, *x),\n",
    "    x0=(nu_bkg_expected, nu_sig_expected, *np.zeros(40), 0),\n",
    "    method=\"SLSQP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hesse = numdifftools.Hessian(\n",
    "    lambda x: L(sample, *x))(fit_result.x)\n",
    "\n",
    "fit_result.covariance =  np.linalg.inv(hesse / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_fit_result(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "    theta(tracking) = {:.2f}\n",
    "    \"\"\".format(ufloat(fit_result.x[-1], fit_result.covariance[-1, -1] ** 0.5))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 0), 0, figname=\"profile-multiplicative-ipar0\")\n",
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 1), 1, figname=\"profile-multiplicative-ipar1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_likelihood_scan(fit_result, *likelihood_scan(fit_result, 42), 42, figname=\"profile-multiplicative-iparTracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_result.x[-1], fit_result.covariance[-1, -1]**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_pulls_on_bins(fit_result, \"nuisance-multiplicative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_template(*sample, fit_result, debug=False, figname=\"template-multiplicative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toy_fit_results = generate_toy(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_toy_fit_results(toy_fit_results, 0, nu_bkg_expected, figname=\"toy-multiplicative-ipar0\")\n",
    "# plot_toy_fit_results(toy_fit_results, 1, nu_sig_expected, figname=\"toy-multiplicative-ipar1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
